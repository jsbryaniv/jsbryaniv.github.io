<!DOCTYPE html>
<html lang="en" class="no-js">
    <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    
    

    
    

    
    

    
    

    <title>Dirichlet Clustring for Clinical Outcome Prediction | J. Shepard Bryan IV</title>
    <meta name="description" content="Here we show how to use Dirichlet Clustering to predict clinical outcomes.">
    
        <meta name="keywords" content="bayesian, coding, prediction">
    

    <!-- Social: Twitter -->
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:title" content="Dirichlet Clustring for Clinical Outcome Prediction | J. Shepard Bryan IV">
    <meta name="twitter:description" content="Here we show how to use Dirichlet Clustering to predict clinical outcomes.">

    
        <meta property="twitter:image" content="https://live.staticflickr.com/65535/53334792868_86038b3779_b.jpg">
    
    
    
        <meta name="twitter:site" content="@shep_iv">
    

    <!-- Social: Facebook / Open Graph -->
    <meta property="og:url" content="http://localhost:4000/dirichlet-for-clinical-outcome-prediction/">
    <meta property="og:title" content="Dirichlet Clustring for Clinical Outcome Prediction | J. Shepard Bryan IV">
    <meta property="og:image" content="https://live.staticflickr.com/65535/53334792868_86038b3779_b.jpg">
    <meta property="og:description" content="Here we show how to use Dirichlet Clustering to predict clinical outcomes.">
    <meta property="og:site_name" content="Shep's Spot">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/favicon.ico" type="image/x-icon" />
    
    <!-- Apple Touch Icons -->
    <link rel="apple-touch-icon" href="/assets/img/icons/apple-touch-icon.png" />
    <link rel="apple-touch-icon" sizes="57x57" href="/assets/img/icons/apple-touch-icon-57x57.png" />
    <link rel="apple-touch-icon" sizes="72x72" href="/assets/img/icons/apple-touch-icon-72x72.png" />
    <link rel="apple-touch-icon" sizes="114x114" href="/assets/img/icons/apple-touch-icon-114x114.png" />
    <link rel="apple-touch-icon" sizes="144x144" href="/assets/img/icons/apple-touch-icon-144x144.png" />
    <link rel="apple-touch-icon" sizes="60x60" href="/assets/img/icons/apple-touch-icon-60x60.png" />
    <link rel="apple-touch-icon" sizes="120x120" href="/assets/img/icons/apple-touch-icon-120x120.png" />
    <link rel="apple-touch-icon" sizes="76x76" href="/assets/img/icons/apple-touch-icon-76x76.png" />
    <link rel="apple-touch-icon" sizes="152x152" href="/assets/img/icons/apple-touch-icon-152x152.png" />

    <!-- Windows 8 Tile Icons -->
    <meta name="application-name" content="J. Shepard Bryan IV">
    <meta name="msapplication-TileColor" content="#141414">
    <meta name="msapplication-square70x70logo" content="smalltile.png" />
    <meta name="msapplication-square150x150logo" content="mediumtile.png" />
    <meta name="msapplication-wide310x150logo" content="widetile.png" />
    <meta name="msapplication-square310x310logo" content="largetile.png" />
    
    <!-- Android Lolipop Theme Color -->
    <meta name="theme-color" content="#141414">

    <!-- Fonts -->
    <link href="https://fonts.googleapis.com/css?family=Titillium+Web:300,400,700" rel="stylesheet">

    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="canonical" href="http://localhost:4000/dirichlet-for-clinical-outcome-prediction/">
    <link rel="alternate" type="application/rss+xml" title="Shep's Spot" href="http://localhost:4000/feed.xml" />

    <!-- Include extra styles -->
    

    <!-- JavaScript enabled/disabled -->
    <script>
        document.querySelector('html').classList.remove('no-js');
    </script>
</head>

    <body class="has-push-menu">
        





        <svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" display="none" version="1.1"><defs><symbol id="icon-menu" viewBox="0 0 1024 1024"><path class="path1" d="M128 213.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 725.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5zM128 469.333h768q17.667 0 30.167 12.5t12.5 30.167-12.5 30.167-30.167 12.5h-768q-17.667 0-30.167-12.5t-12.5-30.167 12.5-30.167 30.167-12.5z"/></symbol><symbol id="icon-search" viewBox="0 0 951 1024"><path class="path1" d="M658.286 475.429q0-105.714-75.143-180.857t-180.857-75.143-180.857 75.143-75.143 180.857 75.143 180.857 180.857 75.143 180.857-75.143 75.143-180.857zM950.857 950.857q0 29.714-21.714 51.429t-51.429 21.714q-30.857 0-51.429-21.714l-196-195.429q-102.286 70.857-228 70.857-81.714 0-156.286-31.714t-128.571-85.714-85.714-128.571-31.714-156.286 31.714-156.286 85.714-128.571 128.571-85.714 156.286-31.714 156.286 31.714 128.571 85.714 85.714 128.571 31.714 156.286q0 125.714-70.857 228l196 196q21.143 21.143 21.143 51.429z"/></symbol><symbol id="icon-close" viewBox="0 0 1000 1000"><path d="M969.8,870.3c27,27.7,27,71.8,0,99.1C955.7,983,937.9,990,920,990c-17.9,0-35.7-7-49.7-20.7L500,599L129.6,969.4C115.6,983,97.8,990,79.9,990s-35.7-7-49.7-20.7c-27-27.3-27-71.4,0-99.1L400.9,500L30.3,129.3c-27-27.3-27-71.4,0-99.1c27.3-27,71.8-27,99.4,0L500,400.9L870.4,30.2c27.7-27,71.8-27,99.4,0c27,27.7,27,71.8,0,99.1L599.1,500L969.8,870.3z"/></symbol><symbol id="icon-twitter" viewBox="0 0 951 1024"><path class="path1" d="M925.714 233.143q-38.286 56-92.571 95.429 0.571 8 0.571 24 0 74.286-21.714 148.286t-66 142-105.429 120.286-147.429 83.429-184.571 31.143q-154.857 0-283.429-82.857 20 2.286 44.571 2.286 128.571 0 229.143-78.857-60-1.143-107.429-36.857t-65.143-91.143q18.857 2.857 34.857 2.857 24.571 0 48.571-6.286-64-13.143-106-63.714t-42-117.429v-2.286q38.857 21.714 83.429 23.429-37.714-25.143-60-65.714t-22.286-88q0-50.286 25.143-93.143 69.143 85.143 168.286 136.286t212.286 56.857q-4.571-21.714-4.571-42.286 0-76.571 54-130.571t130.571-54q80 0 134.857 58.286 62.286-12 117.143-44.571-21.143 65.714-81.143 101.714 53.143-5.714 106.286-28.571z"/></symbol><symbol id="icon-facebook" viewBox="0 0 585 1024"><path class="path1" d="M548 6.857v150.857h-89.714q-49.143 0-66.286 20.571t-17.143 61.714v108h167.429l-22.286 169.143h-145.143v433.714h-174.857v-433.714h-145.714v-169.143h145.714v-124.571q0-106.286 59.429-164.857t158.286-58.571q84 0 130.286 6.857z"/></symbol><symbol id="icon-clock" viewBox="0 0 1000 1000"><path d="M500,10C229.8,10,10,229.8,10,500c0,270.2,219.8,490,490,490c270.2,0,490-219.8,490-490C990,229.8,770.2,10,500,10z M500,910.2c-226.2,0-410.2-184-410.2-410.2c0-226.2,184-410.2,410.2-410.2c226.2,0,410.2,184,410.2,410.2C910.2,726.1,726.2,910.2,500,910.2z M753.1,374c8.2,11.9,5.2,28.1-6.6,36.3L509.9,573.7c-4.4,3.1-9.6,4.6-14.8,4.6c-4.1,0-8.3-1-12.1-3c-8.6-4.5-14-13.4-14-23.1V202.5c0-14.4,11.7-26.1,26.1-26.1c14.4,0,26.1,11.7,26.1,26.1v300l195.6-135.1C728.7,359.2,744.9,362.1,753.1,374z"/></symbol><symbol id="icon-calendar" viewBox="0 0 1000 1000"><path d="M920,500v420H80V500H920 M990,430H10v490c0,38.7,31.3,70,70,70h840c38.7,0,70-31.3,70-70V430L990,430z"/><path d="M850,80v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80H360v105c0,57.9-47.2,105-105,105c-58,0-105-47.1-105-105V80C72.8,80,10,142.7,10,220v140h980V220C990,142.7,927.2,80,850,80z"/><path d="M255,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C290,25.8,274.3,10,255,10z"/><path d="M745,10c-19.3,0-35,15.8-35,35v140c0,19.2,15.7,35,35,35c19.3,0,35-15.8,35-35V45C780,25.8,764.3,10,745,10z"/></symbol><symbol id="icon-github" viewBox="0 0 12 14"><path d="M6 1q1.633 0 3.012 0.805t2.184 2.184 0.805 3.012q0 1.961-1.145 3.527t-2.957 2.168q-0.211 0.039-0.312-0.055t-0.102-0.234q0-0.023 0.004-0.598t0.004-1.051q0-0.758-0.406-1.109 0.445-0.047 0.801-0.141t0.734-0.305 0.633-0.52 0.414-0.82 0.16-1.176q0-0.93-0.617-1.609 0.289-0.711-0.062-1.594-0.219-0.070-0.633 0.086t-0.719 0.344l-0.297 0.187q-0.727-0.203-1.5-0.203t-1.5 0.203q-0.125-0.086-0.332-0.211t-0.652-0.301-0.664-0.105q-0.352 0.883-0.062 1.594-0.617 0.68-0.617 1.609 0 0.664 0.16 1.172t0.41 0.82 0.629 0.523 0.734 0.305 0.801 0.141q-0.305 0.281-0.383 0.805-0.164 0.078-0.352 0.117t-0.445 0.039-0.512-0.168-0.434-0.488q-0.148-0.25-0.379-0.406t-0.387-0.187l-0.156-0.023q-0.164 0-0.227 0.035t-0.039 0.090 0.070 0.109 0.102 0.094l0.055 0.039q0.172 0.078 0.34 0.297t0.246 0.398l0.078 0.18q0.102 0.297 0.344 0.48t0.523 0.234 0.543 0.055 0.434-0.027l0.18-0.031q0 0.297 0.004 0.691t0.004 0.426q0 0.141-0.102 0.234t-0.312 0.055q-1.812-0.602-2.957-2.168t-1.145-3.527q0-1.633 0.805-3.012t2.184-2.184 3.012-0.805zM2.273 9.617q0.023-0.055-0.055-0.094-0.078-0.023-0.102 0.016-0.023 0.055 0.055 0.094 0.070 0.047 0.102-0.016zM2.516 9.883q0.055-0.039-0.016-0.125-0.078-0.070-0.125-0.023-0.055 0.039 0.016 0.125 0.078 0.078 0.125 0.023zM2.75 10.234q0.070-0.055 0-0.148-0.062-0.102-0.133-0.047-0.070 0.039 0 0.141t0.133 0.055zM3.078 10.562q0.062-0.062-0.031-0.148-0.094-0.094-0.156-0.023-0.070 0.062 0.031 0.148 0.094 0.094 0.156 0.023zM3.523 10.758q0.023-0.086-0.102-0.125-0.117-0.031-0.148 0.055t0.102 0.117q0.117 0.047 0.148-0.047zM4.016 10.797q0-0.102-0.133-0.086-0.125 0-0.125 0.086 0 0.102 0.133 0.086 0.125 0 0.125-0.086zM4.469 10.719q-0.016-0.086-0.141-0.070-0.125 0.023-0.109 0.117t0.141 0.062 0.109-0.109z"></path></symbol><symbol id="icon-medium" viewBox="0 0 1000 1000"><path d="M336.5,240.2v641.5c0,9.1-2.3,16.9-6.8,23.2s-11.2,9.6-20,9.6c-6.2,0-12.2-1.5-18-4.4L37.3,782.7c-7.7-3.6-14.1-9.8-19.4-18.3S10,747.4,10,739V115.5c0-7.3,1.8-13.5,5.5-18.6c3.6-5.1,8.9-7.7,15.9-7.7c5.1,0,13.1,2.7,24.1,8.2l279.5,140C335.9,238.6,336.5,239.5,336.5,240.2L336.5,240.2z M371.5,295.5l292,473.6l-292-145.5V295.5z M990,305.3v576.4c0,9.1-2.6,16.5-7.7,22.1c-5.1,5.7-12,8.5-20.8,8.5s-17.3-2.4-25.7-7.1L694.7,784.9L990,305.3z M988.4,239.7c0,1.1-46.8,77.6-140.3,229.4C754.6,621,699.8,709.8,683.8,735.7L470.5,389l177.2-288.2c6.2-10.2,15.7-15.3,28.4-15.3c5.1,0,9.8,1.1,14.2,3.3l295.9,147.7C987.6,237.1,988.4,238.2,988.4,239.7L988.4,239.7z"/></symbol><symbol id="icon-instagram" viewBox="0 0 489.84 489.84"><path d="M249.62,50.46c65.4,0,73.14.25,99,1.43C372.47,53,385.44,57,394.07,60.32a75.88,75.88,0,0,1,28.16,18.32,75.88,75.88,0,0,1,18.32,28.16c3.35,8.63,7.34,21.6,8.43,45.48,1.18,25.83,1.43,33.57,1.43,99s-0.25,73.14-1.43,99c-1.09,23.88-5.08,36.85-8.43,45.48a81.11,81.11,0,0,1-46.48,46.48c-8.63,3.35-21.6,7.34-45.48,8.43-25.82,1.18-33.57,1.43-99,1.43s-73.15-.25-99-1.43c-23.88-1.09-36.85-5.08-45.48-8.43A75.88,75.88,0,0,1,77,423.86,75.88,75.88,0,0,1,58.69,395.7c-3.35-8.63-7.34-21.6-8.43-45.48-1.18-25.83-1.43-33.57-1.43-99s0.25-73.14,1.43-99c1.09-23.88,5.08-36.85,8.43-45.48A75.88,75.88,0,0,1,77,78.64a75.88,75.88,0,0,1,28.16-18.32c8.63-3.35,21.6-7.34,45.48-8.43,25.83-1.18,33.57-1.43,99-1.43m0-44.13c-66.52,0-74.86.28-101,1.47s-43.87,5.33-59.45,11.38A120.06,120.06,0,0,0,45.81,47.44,120.06,120.06,0,0,0,17.56,90.82C11.5,106.4,7.36,124.2,6.17,150.27s-1.47,34.46-1.47,101,0.28,74.86,1.47,101,5.33,43.87,11.38,59.45a120.06,120.06,0,0,0,28.25,43.38,120.06,120.06,0,0,0,43.38,28.25c15.58,6.05,33.38,10.19,59.45,11.38s34.46,1.47,101,1.47,74.86-.28,101-1.47,43.87-5.33,59.45-11.38a125.24,125.24,0,0,0,71.63-71.63c6.05-15.58,10.19-33.38,11.38-59.45s1.47-34.46,1.47-101-0.28-74.86-1.47-101-5.33-43.87-11.38-59.45a120.06,120.06,0,0,0-28.25-43.38,120.06,120.06,0,0,0-43.38-28.25C394.47,13.13,376.67,9,350.6,7.8s-34.46-1.47-101-1.47h0Z" transform="translate(-4.7 -6.33)" /><path d="M249.62,125.48A125.77,125.77,0,1,0,375.39,251.25,125.77,125.77,0,0,0,249.62,125.48Zm0,207.41a81.64,81.64,0,1,1,81.64-81.64A81.64,81.64,0,0,1,249.62,332.89Z" transform="translate(-4.7 -6.33)"/><circle cx="375.66" cy="114.18" r="29.39" /></symbol><symbol id="icon-linkedin" viewBox="0 0 12 14"><path d="M2.727 4.883v7.742h-2.578v-7.742h2.578zM2.891 2.492q0.008 0.57-0.395 0.953t-1.059 0.383h-0.016q-0.641 0-1.031-0.383t-0.391-0.953q0-0.578 0.402-0.957t1.051-0.379 1.039 0.379 0.398 0.957zM12 8.187v4.437h-2.57v-4.141q0-0.82-0.316-1.285t-0.988-0.465q-0.492 0-0.824 0.27t-0.496 0.668q-0.086 0.234-0.086 0.633v4.32h-2.57q0.016-3.117 0.016-5.055t-0.008-2.313l-0.008-0.375h2.57v1.125h-0.016q0.156-0.25 0.32-0.438t0.441-0.406 0.68-0.34 0.895-0.121q1.336 0 2.148 0.887t0.813 2.598z"></path></symbol><symbol id="icon-heart" viewBox="0 0 34 30"><path d="M17,29.7 L16.4,29.2 C3.5,18.7 0,15 0,9 C0,4 4,0 9,0 C13.1,0 15.4,2.3 17,4.1 C18.6,2.3 20.9,0 25,0 C30,0 34,4 34,9 C34,15 30.5,18.7 17.6,29.2 L17,29.7 Z M9,2 C5.1,2 2,5.1 2,9 C2,14.1 5.2,17.5 17,27.1 C28.8,17.5 32,14.1 32,9 C32,5.1 28.9,2 25,2 C21.5,2 19.6,4.1 18.1,5.8 L17,7.1 L15.9,5.8 C14.4,4.1 12.5,2 9,2 Z" id="Shape"></path></symbol><symbol id="icon-arrow-right" viewBox="0 0 25.452 25.452"><path d="M4.471,24.929v-2.004l12.409-9.788c0.122-0.101,0.195-0.251,0.195-0.411c0-0.156-0.073-0.31-0.195-0.409L4.471,2.526V0.522c0-0.2,0.115-0.384,0.293-0.469c0.18-0.087,0.396-0.066,0.552,0.061l15.47,12.202c0.123,0.1,0.195,0.253,0.195,0.409c0,0.16-0.072,0.311-0.195,0.411L5.316,25.34c-0.155,0.125-0.372,0.147-0.552,0.061C4.586,25.315,4.471,25.13,4.471,24.929z"/></symbol><symbol id="icon-star" viewBox="0 0 48 48"><path fill="currentColor" d="M44,24c0,11.045-8.955,20-20,20S4,35.045,4,24S12.955,4,24,4S44,12.955,44,24z"/><path fill="#ffffff" d="M24,11l3.898,7.898l8.703,1.301l-6.301,6.102l1.5,8.699L24,30.898L16.199,35l1.5-8.699l-6.301-6.102  l8.703-1.301L24,11z"/></symbol><symbol id="icon-read" viewBox="0 0 32 32"><path fill="currentColor" d="M29,4H3C1.343,4,0,5.343,0,7v18c0,1.657,1.343,3,3,3h10c0,0.552,0.448,1,1,1h4c0.552,0,1-0.448,1-1h10  c1.657,0,3-1.343,3-3V7C32,5.343,30.657,4,29,4z M29,5v20H18.708c-0.618,0-1.236,0.146-1.789,0.422l-0.419,0.21V5H29z M15.5,5  v20.632l-0.419-0.21C14.528,25.146,13.91,25,13.292,25H3V5H15.5z M31,25c0,1.103-0.897,2-2,2H18v1h-4v-1H3c-1.103,0-2-0.897-2-2V7  c0-0.737,0.405-1.375,1-1.722V25c0,0.552,0.448,1,1,1h10.292c0.466,0,0.925,0.108,1.342,0.317l0.919,0.46  c0.141,0.07,0.294,0.106,0.447,0.106c0.153,0,0.306-0.035,0.447-0.106l0.919-0.46C17.783,26.108,18.242,26,18.708,26H29  c0.552,0,1-0.448,1-1V5.278C30.595,5.625,31,6.263,31,7V25z M6,12.5C6,12.224,6.224,12,6.5,12h5c0.276,0,0.5,0.224,0.5,0.5  S11.776,13,11.5,13h-5C6.224,13,6,12.776,6,12.5z M6,14.5C6,14.224,6.224,14,6.5,14h5c0.276,0,0.5,0.224,0.5,0.5S11.776,15,11.5,15  h-5C6.224,15,6,14.776,6,14.5z M6,16.5C6,16.224,6.224,16,6.5,16h5c0.276,0,0.5,0.224,0.5,0.5S11.776,17,11.5,17h-5  C6.224,17,6,16.776,6,16.5z M20,12.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,13,25.5,13h-5  C20.224,13,20,12.776,20,12.5z M20,14.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,15,25.5,15h-5  C20.224,15,20,14.776,20,14.5z M20,16.5c0-0.276,0.224-0.5,0.5-0.5h5c0.276,0,0.5,0.224,0.5,0.5S25.776,17,25.5,17h-5  C20.224,17,20,16.776,20,16.5z"></path></symbol></defs></svg>

        <header class="bar-header">
    <a id="menu" role="button">
        <svg id="open" class="icon-menu"><use xlink:href="#icon-menu"></use></svg>
    </a>
    <h1 class="logo">
        <a href="/">
            
                J. Shepard Bryan IV <span class="version">v3.1.2</span>
            
        </a>
    </h1>
    <a id="search" class="dosearch" role="button">
        <svg class="icon-search"><use xlink:href="#icon-search"></use></svg>
    </a>
    
        <a href="https://github.com/thiagorossener/jekflix-template" class="get-theme" role="button">
            Get this theme!
        </a>
    
</header>

<div id="mask" class="overlay"></div>

<aside class="sidebar" id="sidebar">
    <nav id="navigation">
      <h2>Menu</h2>
      <ul>
  
    
      <li>
        <a href="http://localhost:4000/">Home</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/about">About</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/contact">Contact</a>
      </li>
    
  
</ul>

    </nav>
</aside>

<div class="search-wrapper">
    <div class="search-form">
        <input type="text" class="search-field" placeholder="Search">
        <svg class="icon-remove-sign"><use xlink:href="#icon-close"></use></svg>
        <ul class="search-results search-list"></ul>
    </div>
</div>



        <section class="post two-columns">
            <article role="article" class="post-content">
                <p class="post-info">
                    
                        <svg class="icon-calendar" id="date"><use xlink:href="#icon-calendar"></use></svg>
                        <time class="date" datetime="2023-11-15T07:26:40-05:00">
                            


November 15, 2023

                        </time>
                    
                    <svg id="clock" class="icon-clock"><use xlink:href="#icon-clock"></use></svg>
                    <span>10 min to read</span>
                </p>
                <h1 class="post-title">Dirichlet Clustring for Clinical Outcome Prediction</h1>
                <p class="post-subtitle">A simple example</p>

                
                    <img src="https://live.staticflickr.com/65535/53334792868_86038b3779_b.jpg" alt="Featured image" class="post-cover">
                

                <!-- Pagination links -->



                <!-- Add your table of contents here -->


                <h1 id="introduction">Introduction</h1>

<p>Clustering is an effective way of making predictions. Clustering works by grouping data points together bases on their features. In the reverse of this, if we know the cluster a data point belongs to, we can make predictions about the data point. In the case of medicine this can be helpful for clinical outcome predicitons since by knowing the cluster a patient belongs to, we can make predictions about their diagnosis.</p>

<p>A clustering tool for diagnosis would work as follows: we take a patient, measure various features about them, find out which cluster they most likely belong to, and then make a prediction about their diagnosis based on the cluster they belong to. This is a simple and effective way of making predictions about a patient’s diagnosis.</p>

<p>However, we cannot deploy this model until we have learned the features of each cluster. Since the cluster that a patient belongs to is often not directly measureable, we must infer the cluster assignments from the data. This is where Bayesian inference comes in. Bayesian inference is a method of inferring unknown variables from data.</p>

<p>In this article we show how to build a clustering model using Dirichlet Clustering in a Bayesian framework. We then apply our model to a dataset to predict the diagnosis of patents. We will provide a high level overview of how the math and the code works, but we wont dive deep. If you want to learn more about the math behind Dirichlet Clustering, I encourage you to leave a comment below check out this <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=LkPGEAAAQBAJ&amp;oi=fnd&amp;pg=PR11&amp;dq=steve+presse+ioannis+sgouralis+data+modeling&amp;ots=uA42nAUM_f&amp;sig=ll9bYjfOmGnMgeJn5QqR-7IM54Q#v=onepage&amp;q=steve%20presse%20ioannis%20sgouralis%20data%20modeling&amp;f=false">textbook</a>.</p>

<h1 id="data">Data</h1>

<p>For this project we are looking at the <a href="https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database">Pima Indians Diabetes Database</a> from Kaggle. The data tracks the medical history of Pima Indians and whether or not they have diabetes.</p>

<p>For each patient we have 8 measured features (Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI,DiabetesPedigreeFunction, and Age) as well as their diabetes diagnosis in the last column(Outcome). We print the first few lines here:</p>

<table>
  <thead>
    <tr>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>For simplicity we will exclude the Pregnancies column as well as any row that has missing information.</p>

<p>In the <code class="language-plaintext highlighter-rouge">main.py</code> file of our GitHub project we load the data and split it into a training and testing set as follows:</p>
<pre><code class="language-pyton">~|#| Load data
data ~|=| np.genfromtxt('data/diabetes.csv', delimiter~|=|',', skip_header~|=|True)
data ~|=| data[:, 1:]                               ~|#| Ignore first column (~|#| pregnancies)
data ~|=| data[~np.any(data[:, :~|-|1] ~|=|~|=| 0, axis~|=|1)]  ~|#| Filter our rows with missing values

~|#| Split data into training and test sets
np.random.shuffle(data)
split ~|=| int(0.8 * data.shape[0])
data_train ~|=| data[:split, :]
data_test ~|=| data[split:, :]
</code></pre>

<p>Now that we have our data we can begin building our model.</p>

<h1 id="model">Model</h1>
<h2 id="equations">Equations</h2>

<p>The foundation of any Bayesian model is the mathematical model that supports it. In this section we will derive the equations that support our model. There are two main parts to the model: the equations that give rise to the cluster assignments and the equations that give rise to the measurements.</p>

<h3 id="cluster-assignments">Cluster Assignments</h3>

<p>First we must choose how many clusters there are. Let $K$ be the number of total clusters, indexed from $k=1,…,K$. Now let $N$ be the number of patients, indexed from $n=1,…,N$. Each patient has a cluster assignment $s_n$ where $s_n$ is the cluster that patient $n$ is assigned to. We can represent this as a vector $\mathbf{s} = (s_1, …, s_N)$.</p>

<p>If we randomly choose a patient they must be assigned to one of the $K$ clusters. Let’s define $\pi_k$ as the probability that a randomly chosen patient is assigned to cluster $k$. In other words, $\pi_k$ represents the portion of the patient population that are in cluster $k$. Since the patient must be assigned to one of the $K$ clusters, we have the constraint that $\sum_{k=1}^K \pi_k = 1$. We can represent this as a vector $\boldsymbol{\pi} = (\pi_1, …, \pi_K)$.</p>

<h3 id="measurements">Measurements</h3>

<p>The next part of our model is to define the feature measurements. Let $F$ be the number of features indexed with $f=1,…,F$. Each feature, $f$, of each class, $k$, will have a mean $\mu_{kf}$ and a variance $\sigma^2<em>{kf}$ . We can define vectors $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ that contain all of the means and variances respectively.
We can tell different classes apart based on their differences in features. Each patient, $n$, will have one measurement per feature, $f$, which we will call $x</em>{nf}$. We can represent all of the measurements as a matrix $\mathbf{X}$ where $\mathbf{X}<em>{nf} = x</em>{nf}$. The probability that a patient has a certain measurement is given by the normal distribution:
\(p(x_{nf} | s_n, \boldsymbol{\mu}, \boldsymbol{\sigma}) = \mathcal{N}(x_{nf} | \mu_{s_nf}, \sigma^2_{s_nf})\)</p>

<p>In additions to measuring the patients features, we also have measure their diagnosis. Let $y_n$ be the diagnosis of patient $n$. The probability that patient $n$ has diabetes is conditioned on their class, $s_n$. Each class has a different probability of having diabetes. Let $\phi_k$ be the probability that a patient in class $k$ has diabetes. We can write this as a vector $\boldsymbol{\phi} = (\phi_1, …, \phi_K)$. All together the probability that a patient has diabetes given their class is:
\(p(y_n | s_n, \boldsymbol{\phi}) = \phi_{s_n}^{y_n} (1 - \phi_{s_n})^{1 - y_n}\)</p>

<h3 id="priors">Priors</h3>

<p>Since we are working in the Bayesian paradigm we must assign a prior distribution over all variables we wish to infer, $\boldsymbol{\pi}$, $\boldsymbol{\mu}$, $\boldsymbol{\sigma}$, and $\boldsymbol{\phi}$.</p>

<p>Starting with $\boldsymbol{\pi}$, the most suitable choice for this distribution is the Dirichlet distribution. The Dirichlet distribution is a distribution over vectors that are constrained to sum to 1. The Dirichlet distribution is parameterized by a vector $\boldsymbol{\alpha} = (\alpha_1, …, \alpha_K)$ where $\alpha_k$ is the concentration of cluster $k$. This Dirichlet distribution is why we call this method Dirichlet Clustering.</p>

<p>For the remaining variables we set priors as follows: $\boldsymbol{\mu}$ is given a gaussian prior, $\boldsymbol{\sigma}$ is given an inverse gamma prior, and $\boldsymbol{\phi}$ is given a beta prior.</p>

<h2 id="code">Code</h2>

<p>Now that we have a mathematical model we can deploying this into code. In <code class="language-plaintext highlighter-rouge">model.py</code> we create a class called <code class="language-plaintext highlighter-rouge">DirichletClustering</code> that contains all of the code for our model. The class has two main methods: <code class="language-plaintext highlighter-rouge">train</code> and <code class="language-plaintext highlighter-rouge">predict</code>. The <code class="language-plaintext highlighter-rouge">train</code> method takes in the training data and learns the parameters of the model. The <code class="language-plaintext highlighter-rouge">predict</code> method takes in the testing data and uses the learned parameters to make predictions.</p>

<p>One of the biggest parts to pay attention to is how we store the variables from our method. We choose to save these as a python dictionary object where each key is the variable name and each value is the assignment.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
  <span class="n">VARIABLES</span> <span class="o">~|=|</span> <span class="p">{</span>
      <span class="o">~|</span><span class="c1">#| Constants
</span>      <span class="s">'n_data'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>             <span class="o">~|</span><span class="c1">#| The number of data entries
</span>      <span class="s">'n_features'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>         <span class="o">~|</span><span class="c1">#| The number of features
</span>      <span class="s">'n_classes'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>            <span class="o">~|</span><span class="c1">#| The maximum number of classes considered
</span>      <span class="o">~|</span><span class="c1">#| Variables
</span>      <span class="s">'P'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>                  <span class="o">~|</span><span class="c1">#| The probability of the variables
</span>      <span class="s">'s'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>                  <span class="o">~|</span><span class="c1">#| The class assignments
</span>      <span class="s">'x'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>                  <span class="o">~|</span><span class="c1">#| The class outcome probabilities
</span>      <span class="s">'mu'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>                 <span class="o">~|</span><span class="c1">#| The feature means
</span>      <span class="s">'sigma'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>              <span class="o">~|</span><span class="c1">#| The feature standard deviations
</span>      <span class="s">'pi'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>                 <span class="o">~|</span><span class="c1">#| The class probabilities
</span>      <span class="o">~|</span><span class="c1">#| Priors
</span>      <span class="s">'x_prior_alpha'</span><span class="p">:</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span>        <span class="o">~|</span><span class="c1">#| The x prior alpha
</span>      <span class="s">'x_prior_beta'</span><span class="p">:</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span>         <span class="o">~|</span><span class="c1">#| The x prior beta
</span>      <span class="s">'mu_prior_mean'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>      <span class="o">~|</span><span class="c1">#| The mu prior mean
</span>      <span class="s">'mu_prior_std'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>       <span class="o">~|</span><span class="c1">#| The mu prior standard deviation
</span>      <span class="s">'sigma_prior_shape'</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>     <span class="o">~|</span><span class="c1">#| The sigma prior shape
</span>      <span class="s">'sigma_prior_scale'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>  <span class="o">~|</span><span class="c1">#| The sigma prior scale
</span>      <span class="s">'pi_prior_conc'</span><span class="p">:</span> <span class="bp">None</span><span class="p">,</span>    <span class="o">~|</span><span class="c1">#| The beta prior concentration
</span>  <span class="p">}</span>

</code></pre></div></div>

<p>We start the variables that can be calibrated with <code class="language-plaintext highlighter-rouge">None</code> values. For example, <code class="language-plaintext highlighter-rouge">mu_prior_mean</code> is the mean of the prior distribution for the feature means, which is different for each feature. Our class has a function <code class="language-plaintext highlighter-rouge">initialize_variables</code> which takes in data and then outputs a variables dictionary with the variables initialized (technically we use a <code class="language-plaintext highlighter-rouge">SimpleNamespace</code> object which is similar to but different from a dictionary).</p>

<h3 id="training-and-prediciton">Training and Prediciton</h3>

<p>The <code class="language-plaintext highlighter-rouge">train</code> method takes in the training data and learns the parameters of the model. Training works by running a gibbs sampler algorithms over the data and learning the most probable value for each parameter.</p>

<p>In <code class="language-plaintext highlighter-rouge">main.py</code> we start by initializing our model with the desired number of clusters, then running the training method:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_classes</span> <span class="o">~|=|</span> <span class="mi">3</span>
<span class="n">model</span> <span class="o">~|=|</span> <span class="n">DirichletClustering</span><span class="p">(</span><span class="n">n_classes</span><span class="p">)</span>
<span class="n">variables</span><span class="p">,</span> <span class="n">samples</span> <span class="o">~|=|</span> <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">(</span><span class="n">data_train</span><span class="p">)</span>
</code></pre></div></div>
<p>The model learns all the variables of the model including the state assignments, feature variables, and outcome probability.</p>

<p>The <code class="language-plaintext highlighter-rouge">predict</code> function then takes in a new dataset and predicts the cluster assignments based on the learned parameters. The model is set up to ignore featues masked with NaN values. In our case we want to predict the diagnosis of the patients, so we mask the last column of the data. We then run the predict function and print the results:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">data_test_masked</span> <span class="o">~|=|</span> <span class="n">data_test</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">data_test_masked</span><span class="p">[:,</span> <span class="o">~|-|</span><span class="mi">1</span><span class="p">]</span> <span class="o">~|=|</span> <span class="n">np</span><span class="p">.</span><span class="n">nan</span>
<span class="n">s_pred</span> <span class="o">~|=|</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data_test_masked</span><span class="p">,</span> <span class="n">variables</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="results">Results</h1>

<p>Our method does not directly predict the diagnosis of the patients. Instead it predicts the cluster that the patient belongs to. We can then use the cluster assignments to make predictions about the diagnosis. For example, if we know that a patient belongs to cluster 1, we can predict that they have diabetes with a probability of $\pi_k$.</p>

<p>In order to evaluate the effectiveness of our model is to compare the predicted probability of diabetes of each cluster, $\phi_k$ to the actual proportion of diabetes diagnosises per cluster.
We do that below
<a data-flickr-embed="true" href="https://www.flickr.com/photos/199612465@N08/53336518169/in/dateposted-public/" title="Figure_1"><img src="https://live.staticflickr.com/65535/53336518169_88d3d3ae41_z.jpg" width="640" height="480" alt="Figure_1" /></a><script async="" src="//embedr.flickr.com/assets/client-code.js" charset="utf-8"></script></p>

<p>As you can see our model does a reasonable job of predicting the probability of diabetes for each cluster. This means that our model is effective at predicting the diagnosis of patients.</p>

<h1 id="discussion">Discussion</h1>

<p>This model is just for demonstration and would need to be improved in several ways before it could be deployed in a real world setting. For example we would need to include: predicting the number of clusters, covariance between features, non-gaussian distribution over clusters, distributions over learned values rather than point measurements, just to name a few. The goal of this article was just to shed light on the potential that Dirichlet clustering has for clinical outcome prediction.</p>

<p>Please feel free to dive deeper into the code on your own, build upon it, or copy it for your own applications. If you have any questions or comments please leave them below.</p>


                <!-- Pagination links -->


            </article>

            
                <aside class="see-also">
                    <h2>See also</h2>
                    <ul>
                        
                        
                        
                    </ul>
                </aside>
            

        </section>

        <!-- Add time bar only for pages without pagination -->
        
            <div class="time-bar" data-minutes="10">
    <span class="time-completed"></span>
    <span class="time-remaining"></span>
    <div class="bar">
        <span class="completed" style="width:0%;"></span>
        <span class="remaining" style="width:100%;"></span>
    </div>
</div>

            <div class="recommendation">
    <div class="message">
        <strong>Why don't you read something next?</strong>
        <div>
            <button>
                <svg><use xlink:href="#icon-arrow-right"></use></svg>
                <span>Go back to top</span>
            </button>
        </div>
    </div>
    
    <a href="" class="post-preview">
        <div class="image">
            
                <img src="/assets/img/off.jpg">
            
        </div>
        <h3 class="title"></h3>
    </a>
</div>

        

        <!-- Show modal if the post is the last one -->
        

        <!-- Show modal before user leaves the page -->
        

        <!-- Add your newsletter subscription form here -->

        <section class="share">
    <h3>Share</h3>
    <a aria-label="Share on Twitter" href="https://twitter.com/intent/tweet?text=&quot;Here we show how to use Dirichlet Clustering to predict clinical outcomes.&quot;%20http://localhost:4000/dirichlet-for-clinical-outcome-prediction/%20via%20&#64;shep_iv&hashtags=bayesian,coding,prediction"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;" title="Share on Twitter">
        <svg class="icon icon-twitter"><use xlink:href="#icon-twitter"></use></svg>
    </a>
    <a aria-label="Share on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/dirichlet-for-clinical-outcome-prediction/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;" title="Share on Facebook">
        <svg class="icon icon-facebook"><use xlink:href="#icon-facebook"></use></svg>
    </a>
</section>

        

  <section class="author">
    <div class="details">
      
        <img class="img-rounded" src="/assets/img/uploads/profile.png" alt="Shep Bryan IV">
      
      <p class="def">Author</p>
      <h3 class="name">
        <a href="/authors/jsbryaniv/">Shep Bryan IV</a>
      </h3>
      <p class="desc">Florida based physicist specializing in applying AI for problems in biology.</p>
      <p>
        
          <a href="https://github.com/jsbryaniv" title="Github">
            <svg><use xlink:href="#icon-github"></use></svg>
          </a>
        
        
        
          <a href="https://twitter.com/shep_iv" title="Twitter">
            <svg><use xlink:href="#icon-twitter"></use></svg>
          </a>
        
        
        
        
          <a href="https://www.linkedin.com/in/shep-bryan-iv" title="LinkedIn">
            <svg><use xlink:href="#icon-linkedin"></use></svg>
          </a>
        
      </p>
    </div>
  </section>

  
  
  
  
  
  
  

  <script type="application/ld+json">
  {
      "@context": "http://schema.org",
      "@type": "Person",
      "name": "Shep Bryan IV",
      
      "image": "/assets/img/uploads/profile.png",
      
      "jobTitle": "Co-Founder of Saguaro AI",
      "url": "http://localhost:4000/authors/jsbryaniv/",
      "sameAs": [
        "https://github.com/jsbryaniv","https://twitter.com/shep_iv","https://www.linkedin.com/in/shep-bryan-iv"
      ]
  }
  </script>


        

<section class="comments">
    <h3>Comments</h3>
    <div id="disqus_thread"></div>
</section>
<script type="text/javascript">
    var disqus_loaded = false;

    function load_disqus()
    {
        disqus_loaded = true;
        var disqus_shortname = 'jsbryaniv';
        var disqus_title = '';
        var disqus_url = '/dirichlet-for-clinical-outcome-prediction/';
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        var ldr = document.getElementById('disqus_loader');
    };
    window.onscroll = function(e) {
        if ((window.innerHeight + window.scrollY) >= (document.body.offsetHeight - 800)) {
            //hit bottom of page
            if (disqus_loaded==false)
                load_disqus()
        }
    };
</script>



        <footer>
    <p>
      
        <a href="https://github.com/jsbryaniv" title="Github">
          <svg><use xlink:href="#icon-github"></use></svg>
        </a>
      
      
      
        <a href="https://twitter.com/shep_iv" title="Twitter">
          <svg><use xlink:href="#icon-twitter"></use></svg>
        </a>
      
      
      
      
        <a href="https://www.linkedin.com/in/shep-bryan-iv" title="LinkedIn">
          <svg><use xlink:href="#icon-linkedin"></use></svg>
        </a>
      
    </p>

    <ul>
  
    
      <li>
        <a href="http://localhost:4000/">Home</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/about">About</a>
      </li>
    
  
    
      <li>
        <a href="http://localhost:4000/contact">Contact</a>
      </li>
    
  
</ul>


    <p>
      <span>Jekflix</span> was made with <svg class="love"><use xlink:href="#icon-heart"></use></svg> by <a href="https://rossener.com" target="_blank" class="creator">Thiago Rossener</a>
    </p>
</footer>









<script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Organization",
  "name": "J. Shepard Bryan IV",
  "description": "A website to showcase the work done by Shep Bryan IV.",
  "url": "http://localhost:4000/",
  "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:4000/assets/img/icons/mediumtile.png",
      "width": "600",
      "height": "315"
  },
  "sameAs": [
    "https://github.com/jsbryaniv","https://twitter.com/shep_iv","https://www.linkedin.com/in/shep-bryan-iv"
  ]
}
</script>

<!-- Include the script that allows Netlify CMS login -->
<script src="https://identity.netlify.com/v1/netlify-identity-widget.js"></script>

<!-- Include the website scripts -->
<script src="/assets/js/scripts.min.js"></script>

<!-- Include Google Analytics script -->
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXX-X"></script>
<script>
  var host = window.location.hostname;
  if (host != 'localhost') {
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-XXXXXXXX-X');
  }
</script>
  


<!-- Include extra scripts -->



        

        
        
        
        
        
        
        
        
        <script>
            MathJax = {
                tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
                }
            };
        </script>
        <script type="text/javascript" id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
        
        <script type="application/ld+json">
        {
            "@context": "http://schema.org",
            "@type": "BlogPosting",
            "name": "Dirichlet Clustring for Clinical Outcome Prediction",
            "headline": "A simple example",
            "description": "Here we show how to use Dirichlet Clustering to predict clinical outcomes.",
            "image": "https://live.staticflickr.com/65535/53334792868_86038b3779_b.jpg",
            "url": "http://localhost:4000/dirichlet-for-clinical-outcome-prediction/",
            "articleBody": "Introduction

Clustering is an effective way of making predictions. Clustering works by grouping data points together bases on their features. In the reverse of this, if we know the cluster a data point belongs to, we can make predictions about the data point. In the case of medicine this can be helpful for clinical outcome predicitons since by knowing the cluster a patient belongs to, we can make predictions about their diagnosis.

A clustering tool for diagnosis would work as follows: we take a patient, measure various features about them, find out which cluster they most likely belong to, and then make a prediction about their diagnosis based on the cluster they belong to. This is a simple and effective way of making predictions about a patient’s diagnosis.

However, we cannot deploy this model until we have learned the features of each cluster. Since the cluster that a patient belongs to is often not directly measureable, we must infer the cluster assignments from the data. This is where Bayesian inference comes in. Bayesian inference is a method of inferring unknown variables from data.

In this article we show how to build a clustering model using Dirichlet Clustering in a Bayesian framework. We then apply our model to a dataset to predict the diagnosis of patents. We will provide a high level overview of how the math and the code works, but we wont dive deep. If you want to learn more about the math behind Dirichlet Clustering, I encourage you to leave a comment below check out this textbook.

Data

For this project we are looking at the Pima Indians Diabetes Database from Kaggle. The data tracks the medical history of Pima Indians and whether or not they have diabetes.

For each patient we have 8 measured features (Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI,DiabetesPedigreeFunction, and Age) as well as their diabetes diagnosis in the last column(Outcome). We print the first few lines here:


  
    
      Pregnancies
      Glucose
      BloodPressure
      SkinThickness
      Insulin
      BMI
      DiabetesPedigreeFunction
      Age
      Outcome
    
  
  
    
      6
      148
      72
      35
      0
      33.6
      0.627
      50
      1
    
    
      1
      85
      66
      29
      0
      26.6
      0.351
      31
      0
    
    
      8
      183
      64
      0
      0
      23.3
      0.672
      32
      1
    
    
      1
      89
      66
      23
      94
      28.1
      0.167
      21
      0
    
    
      0
      137
      40
      35
      168
      43.1
      2.288
      33
      1
    
  


For simplicity we will exclude the Pregnancies column as well as any row that has missing information.

In the main.py file of our GitHub project we load the data and split it into a training and testing set as follows:
~|#| Load data
data ~|=| np.genfromtxt(&apos;data/diabetes.csv&apos;, delimiter~|=|&apos;,&apos;, skip_header~|=|True)
data ~|=| data[:, 1:]                               ~|#| Ignore first column (~|#| pregnancies)
data ~|=| data[~np.any(data[:, :~|-|1] ~|=|~|=| 0, axis~|=|1)]  ~|#| Filter our rows with missing values

~|#| Split data into training and test sets
np.random.shuffle(data)
split ~|=| int(0.8 * data.shape[0])
data_train ~|=| data[:split, :]
data_test ~|=| data[split:, :]


Now that we have our data we can begin building our model.

Model
Equations

The foundation of any Bayesian model is the mathematical model that supports it. In this section we will derive the equations that support our model. There are two main parts to the model: the equations that give rise to the cluster assignments and the equations that give rise to the measurements.

Cluster Assignments

First we must choose how many clusters there are. Let $K$ be the number of total clusters, indexed from $k=1,…,K$. Now let $N$ be the number of patients, indexed from $n=1,…,N$. Each patient has a cluster assignment $s_n$ where $s_n$ is the cluster that patient $n$ is assigned to. We can represent this as a vector $\mathbf{s} = (s_1, …, s_N)$.

If we randomly choose a patient they must be assigned to one of the $K$ clusters. Let’s define $\pi_k$ as the probability that a randomly chosen patient is assigned to cluster $k$. In other words, $\pi_k$ represents the portion of the patient population that are in cluster $k$. Since the patient must be assigned to one of the $K$ clusters, we have the constraint that $\sum_{k=1}^K \pi_k = 1$. We can represent this as a vector $\boldsymbol{\pi} = (\pi_1, …, \pi_K)$.

Measurements

The next part of our model is to define the feature measurements. Let $F$ be the number of features indexed with $f=1,…,F$. Each feature, $f$, of each class, $k$, will have a mean $\mu_{kf}$ and a variance $\sigma^2{kf}$ . We can define vectors $\boldsymbol{\mu}$ and $\boldsymbol{\sigma}$ that contain all of the means and variances respectively.
We can tell different classes apart based on their differences in features. Each patient, $n$, will have one measurement per feature, $f$, which we will call $x{nf}$. We can represent all of the measurements as a matrix $\mathbf{X}$ where $\mathbf{X}{nf} = x{nf}$. The probability that a patient has a certain measurement is given by the normal distribution:
\(p(x_{nf} | s_n, \boldsymbol{\mu}, \boldsymbol{\sigma}) = \mathcal{N}(x_{nf} | \mu_{s_nf}, \sigma^2_{s_nf})\)

In additions to measuring the patients features, we also have measure their diagnosis. Let $y_n$ be the diagnosis of patient $n$. The probability that patient $n$ has diabetes is conditioned on their class, $s_n$. Each class has a different probability of having diabetes. Let $\phi_k$ be the probability that a patient in class $k$ has diabetes. We can write this as a vector $\boldsymbol{\phi} = (\phi_1, …, \phi_K)$. All together the probability that a patient has diabetes given their class is:
\(p(y_n | s_n, \boldsymbol{\phi}) = \phi_{s_n}^{y_n} (1 - \phi_{s_n})^{1 - y_n}\)

Priors

Since we are working in the Bayesian paradigm we must assign a prior distribution over all variables we wish to infer, $\boldsymbol{\pi}$, $\boldsymbol{\mu}$, $\boldsymbol{\sigma}$, and $\boldsymbol{\phi}$.

Starting with $\boldsymbol{\pi}$, the most suitable choice for this distribution is the Dirichlet distribution. The Dirichlet distribution is a distribution over vectors that are constrained to sum to 1. The Dirichlet distribution is parameterized by a vector $\boldsymbol{\alpha} = (\alpha_1, …, \alpha_K)$ where $\alpha_k$ is the concentration of cluster $k$. This Dirichlet distribution is why we call this method Dirichlet Clustering.

For the remaining variables we set priors as follows: $\boldsymbol{\mu}$ is given a gaussian prior, $\boldsymbol{\sigma}$ is given an inverse gamma prior, and $\boldsymbol{\phi}$ is given a beta prior.

Code

Now that we have a mathematical model we can deploying this into code. In model.py we create a class called DirichletClustering that contains all of the code for our model. The class has two main methods: train and predict. The train method takes in the training data and learns the parameters of the model. The predict method takes in the testing data and uses the learned parameters to make predictions.

One of the biggest parts to pay attention to is how we store the variables from our method. We choose to save these as a python dictionary object where each key is the variable name and each value is the assignment.


  VARIABLES ~|=| {
      ~|#| Constants
      &apos;n_data&apos;: None,             ~|#| The number of data entries
      &apos;n_features&apos;: None,         ~|#| The number of features
      &apos;n_classes&apos;: 2,            ~|#| The maximum number of classes considered
      ~|#| Variables
      &apos;P&apos;: None,                  ~|#| The probability of the variables
      &apos;s&apos;: None,                  ~|#| The class assignments
      &apos;x&apos;: None,                  ~|#| The class outcome probabilities
      &apos;mu&apos;: None,                 ~|#| The feature means
      &apos;sigma&apos;: None,              ~|#| The feature standard deviations
      &apos;pi&apos;: None,                 ~|#| The class probabilities
      ~|#| Priors
      &apos;x_prior_alpha&apos;: .1,        ~|#| The x prior alpha
      &apos;x_prior_beta&apos;: .1,         ~|#| The x prior beta
      &apos;mu_prior_mean&apos;: None,      ~|#| The mu prior mean
      &apos;mu_prior_std&apos;: None,       ~|#| The mu prior standard deviation
      &apos;sigma_prior_shape&apos;: 2,     ~|#| The sigma prior shape
      &apos;sigma_prior_scale&apos;: None,  ~|#| The sigma prior scale
      &apos;pi_prior_conc&apos;: None,    ~|#| The beta prior concentration
  }



We start the variables that can be calibrated with None values. For example, mu_prior_mean is the mean of the prior distribution for the feature means, which is different for each feature. Our class has a function initialize_variables which takes in data and then outputs a variables dictionary with the variables initialized (technically we use a SimpleNamespace object which is similar to but different from a dictionary).

Training and Prediciton

The train method takes in the training data and learns the parameters of the model. Training works by running a gibbs sampler algorithms over the data and learning the most probable value for each parameter.

In main.py we start by initializing our model with the desired number of clusters, then running the training method:
n_classes ~|=| 3
model ~|=| DirichletClustering(n_classes)
variables, samples ~|=| model.train(data_train)

The model learns all the variables of the model including the state assignments, feature variables, and outcome probability.

The predict function then takes in a new dataset and predicts the cluster assignments based on the learned parameters. The model is set up to ignore featues masked with NaN values. In our case we want to predict the diagnosis of the patients, so we mask the last column of the data. We then run the predict function and print the results:

data_test_masked ~|=| data_test.copy()
data_test_masked[:, ~|-|1] ~|=| np.nan
s_pred ~|=| model.predict(data_test_masked, variables)


Results

Our method does not directly predict the diagnosis of the patients. Instead it predicts the cluster that the patient belongs to. We can then use the cluster assignments to make predictions about the diagnosis. For example, if we know that a patient belongs to cluster 1, we can predict that they have diabetes with a probability of $\pi_k$.

In order to evaluate the effectiveness of our model is to compare the predicted probability of diabetes of each cluster, $\phi_k$ to the actual proportion of diabetes diagnosises per cluster.
We do that below


As you can see our model does a reasonable job of predicting the probability of diabetes for each cluster. This means that our model is effective at predicting the diagnosis of patients.

Discussion

This model is just for demonstration and would need to be improved in several ways before it could be deployed in a real world setting. For example we would need to include: predicting the number of clusters, covariance between features, non-gaussian distribution over clusters, distributions over learned values rather than point measurements, just to name a few. The goal of this article was just to shed light on the potential that Dirichlet clustering has for clinical outcome prediction.

Please feel free to dive deeper into the code on your own, build upon it, or copy it for your own applications. If you have any questions or comments please leave them below.
",
            "wordcount": "1970",
            "inLanguage": "en",
            "dateCreated": "2023-11-15/",
            "datePublished": "2023-11-15/",
            "dateModified": "2023-11-15/",
            "author": {
                "@type": "Person",
                "name": "Shep Bryan IV",
                
                "image": "/assets/img/uploads/profile.png",
                
                "jobTitle": "Co-Founder of Saguaro AI",
                "url": "http://localhost:4000/authors/jsbryaniv/",
                "sameAs": [
                    "https://github.com/jsbryaniv","https://twitter.com/shep_iv","https://www.linkedin.com/in/shep-bryan-iv"
                ]
            },
            "publisher": {
                "@type": "Organization",
                "name": "J. Shepard Bryan IV",
                "url": "http://localhost:4000/",
                "logo": {
                    "@type": "ImageObject",
                    "url": "http://localhost:4000/assets/img/blog-image.png",
                    "width": "600",
                    "height": "315"
                }
            },
            "mainEntityOfPage": "True",
            "genre": "math",
            "articleSection": "math",
            "keywords": ["bayesian","coding","prediction"]
        }
        </script>
    </body>
</html>
